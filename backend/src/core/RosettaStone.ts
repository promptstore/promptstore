import uuid from 'uuid';

import logger from '../logger';

import { SchemishConverter } from './Schemish';
import {
  OpenAIMessageImpl,
  UserMessage,
  AssistantMessage,
  SystemMessage,
} from './openai';
import {
  OpenAIMessage,
  OpenAICompletionResponse,
  OpenAIChatCompletionResponse,
} from './openai_types';
import {
  PaLMChatResponse,
  PaLMCompletionResponse,
  PaLMEmbeddingResponse,
  PaLMExample,
  PaLMMessage,
} from './vertexai_types';

export const PARA_DELIM = '\n\n';

/*** universal superset ************/

interface CitationSource {
  start_index?: number;  // Start of segment of the response that is attributed to this source. Index indicates the start of the segment, measured in bytes.
  end_index?: number;  // End of the attributed segment, exclusive.
  uri?: string;  // URI that is attributed as a source for a portion of the text.
  license_?: string;  // License for the GitHub project that is attributed as a source for segment.  License info is required for code citations.
}

export interface CitationMetadata {
  citation_sources: CitationSource[];
}

interface ContextChunk {
  content: string;
  author?: string;
  citation_metadata?: CitationMetadata;
}

interface AdditionalContext {
  content?: string;
  chunks?: ContextChunk[];
}

export interface ChatRequestContext {
  system_prompt?: string;
  additional_context?: AdditionalContext;
}

export enum MessageRole {
  system = 'system',
  user = 'user',
  assistant = 'assistant',
  function = 'function',
}

export interface Message {
  role: MessageRole;
  content: string;
  name?: string;
  function_call?: object;
  citation_metadata?: CitationMetadata;
}

interface FewShotLearningExample {
  input: Message;
  output: Message;
}

interface ChatPrompt {
  context?: ChatRequestContext;
  examples?: FewShotLearningExample[];
  history?: Message[];
  messages: Message[];
}

export interface ModelParams {
  max_tokens?: number;  // The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length. Defaults to inf.
  n?: number;  // How many chat completion choices to generate for each input message. Defaults to 1.
  temperature?: number;  // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1. Alter this or top_p but not both.
  top_p?: number;  // An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1. Alter this or temperature but not both.
  stop?: string | string[];  // Up to 4 sequences where the API will stop generating further tokens. Defaults to null.
  presence_penalty?: number;  // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0.
  frequency_penalty?: number;  // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Defaults to 0.
  logit_bias?: object;  // Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. Defaults to null.
}

interface ChatModelParams extends ModelParams {
  top_k?: number;
}

export interface Function {
  name: string;  // The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
  description?: string;  // A description of what the function does, used by the model to choose when and how to call the function.
  parameters: object;  // JSONSchema document that describes the function and arguments.
}

export enum FunctionCallType {
  none = 'none',  // "none" means the model does not call a function, and responds to the end-user.
  auto = 'auto',  // "auto" means the model can pick between an end-user or calling a function.
}

enum HarmBlockThreshold {
  HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0,
  BLOCK_LOW_AND_ABOVE = 1,
  BLOCK_MEDIUM_AND_ABOVE = 2,
  BLOCK_ONLY_HIGH = 3,
  BLOCK_NONE = 4,
}

export interface SafetySetting {
  category: string;  // The category for this setting.
  threshold: HarmBlockThreshold;  // Controls the probability threshold at which harm is blocked.
}

export interface ChatRequest {
  model: string;
  prompt: ChatPrompt;
  model_params?: ChatModelParams;
  functions?: Function[];  // A list of functions the model may generate JSON inputs for.
  function_call?: FunctionCallType | object;
  best_of?: number;
  stream?: boolean;
  user?: string;
  safety_settings?: SafetySetting[];
}

export interface ProviderRequest {
  provider: string;
  request: ChatRequest;
};

export interface SafetyRating {
  category: string;
  probability: string;
}

interface ChatCompletionChoice {
  finish_reason?: string;
  index: number;
  message: Message;
  safety_ratings?: SafetyRating[];  // Ratings for the safety of a response. There is at most one rating per category.
  logprobs?: object;
}

export interface ChatCompletionUsage {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
}

enum BlockedReason {
  BLOCKED_REASON_UNSPECIFIED = 0,
  SAFETY = 1,
  OTHER = 2,
}

export interface ContentFilter {
  reason: BlockedReason;
  message: string;
}

export interface SafetyFeedback {
  rating: SafetyRating;  // Safety rating evaluated from content.
  setting: SafetySetting;  // Safety setting applied to the request.
}

export interface ChatResponse {
  id: string;
  created: Date;
  model?: string;
  choices: ChatCompletionChoice[];
  usage?: ChatCompletionUsage;
  n: number;
  filters?: ContentFilter[];
  safetyFeedback?: SafetyFeedback[];
}

export interface ResponseMetadata {
  prompts?: Message[];
}

export interface EmbeddingRequest {
  model: string;  // ID of the model to use. 
  input: string | string[];  // Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. Each input must not exceed the max input tokens for the model (8191 tokens for text-embedding-ada-002).
  user?: string;  // A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
}

export interface EmbeddingResponse {
  index: number;  // The index of the embedding in the list of embeddings.
  object: string;  // The object type, which is always "embedding".
  embedding: number[];  // The embedding vector, which is a list of floats. The length of vector depends on the model.
}

/*** ************/

/*** translate to vertexai ************/

export function toVertexAIChatRequest(request: ChatRequest) {
  const {
    prompt,
    model,
    model_params,
    functions,
  } = request;
  const {
    temperature,
    top_k,
    top_p,
    n,
  } = model_params;
  const messages: PaLMMessage[] = [];
  if (prompt.history) {
    messages.push(...prompt.history.map(m => ({
      author: m.name,
      content: m.content,
      citation_metadata: m.citation_metadata,
    })));
  }
  messages.push(...prompt.messages.map(m => ({
    author: m.name,
    content: m.content,
    citation_metadata: m.citation_metadata,
  })));
  let examples: PaLMExample[];
  if (prompt.examples) {
    examples = prompt.examples.map(({ input, output }) => ({
      input: toPaLMMessage(input),
      output: toPaLMMessage(output),
    }));
  }
  let context: string;
  if (prompt.context) {
    context = createSystemPrompt(prompt.context, functions);
  }
  return {
    model,
    prompt: {
      context,
      examples,
      messages,
    },
    temperature,
    candidate_count: n,
    top_p,
    top_k,
  };
}

export function fromVertexAIChatResponse(response: PaLMChatResponse) {
  const {
    candidates,
    filters,
  } = response;
  return {
    id: uuid.v4(),
    created: new Date(),
    choices: candidates.map((c, i) => ({
      index: i,
      message: {
        role: MessageRole.assistant,
        content: c.content,
        name: c.author,
        citation_metadata: c.citation_metadata,
      }
    })),
    n: candidates.length,
    filters,
  };
}

export function toVertexAICompletionRequest(request: ChatRequest) {
  const {
    functions,
    model,
    model_params,
    safety_settings,
  } = request;
  const messages = createOpenAIMessages(request.prompt, functions);
  const text = messages.map(m => m.content).join(PARA_DELIM);
  const {
    temperature,
    top_p,
    top_k,
    n,
    stop,
    max_tokens,
  } = model_params;
  return {
    model,
    prompt: { text },
    temperature,
    candidate_count: n,
    max_output_tokens: max_tokens,
    top_p,
    top_k,
    safety_settings,
    stop_sequences: stop,
  };
}

export function fromVertexAICompletionResponse(response: PaLMCompletionResponse) {
  const {
    candidates,
    filters,
    safety_feedback,
  } = response;
  return {
    id: uuid.v4(),
    created: new Date(),
    choices: candidates.map((c, i) => ({
      index: i,
      message: {
        role: 'assistant',
        content: c.output,
      },
      safetyRatings: c.safety_ratings,
    })),
    n: candidates.length,
    safetyFeedback: safety_feedback,
    filters,
  };
}

export function toVertexAIEmbeddingRequest(request: EmbeddingRequest) {
  const { model, input } = request;
  let text: string;
  if (Array.isArray(input)) {
    text = input.join(PARA_DELIM);
  } else {
    text = input;
  }
  return { model, text };
}

export function fromVertexAIEmbeddingResponse(response: PaLMEmbeddingResponse) {
  return {
    index: 0,
    object: 'embedding',
    embedding: response.embedding.value,
  };
}

function toPaLMMessage(message: Message) {
  return {
    author: message.name,
    content: message.content,
    citation_metadata: message.citation_metadata,
  };
}

/*** ************/

/*** translate to openai ************/

export function toOpenAIChatRequest(request: ChatRequest) {
  const {
    functions,
    function_call,
    model,
    model_params,
    stream,
    user,
  } = request;
  const {
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  } = model_params;
  const messages = createOpenAIMessages(request.prompt);
  return {
    model,
    messages,
    functions,
    function_call,
    stream,
    user,
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  };
}

export function fromOpenAIChatResponse(response: OpenAIChatCompletionResponse) {
  const {
    id,
    created,
    model,
    choices,
    usage,
  } = response;
  return {
    id,
    created,
    model,
    choices: choices.map(c => ({
      finish_reason: c.finish_reason,
      index: c.index,
      message: {
        role: c.message.role,
        content: c.message.content,
        name: c.message.name,
        function_call: c.message.function_call,
      }
    })),
    usage,
  };
}

export function toOpenAICompletionRequest(request: ChatRequest) {
  const {
    functions,
    model,
    model_params,
    best_of,
    stream,
    user,
  } = request;
  const messages = createOpenAIMessages(request.prompt, functions);
  const prompt = messages.map(m => m.content).join(PARA_DELIM);
  const {
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  } = model_params;
  return {
    model,
    prompt,
    stream,
    user,
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
    best_of,
  };
}

export function fromOpenAICompletionResponse(response: OpenAICompletionResponse) {
  const {
    id,
    created,
    model,
    choices,
    usage,
  } = response;
  return {
    id,
    created,
    model,
    choices: choices.map(c => ({
      finish_reason: c.finish_reason,
      index: c.index,
      message: {
        role: 'assistant',
        content: c.text,
      },
      logprobs: c.logprobs,
    })),
    usage,
  };
}

/*** ************/

/*** utility ************/

function createOpenAIMessages(prompt: ChatPrompt, functions?: Function[]) {
  const messages: OpenAIMessage[] = [];
  if (prompt.context) {
    const systemPrompt = createSystemPrompt(prompt.context, functions);
    if (systemPrompt) {
      messages.push(new SystemMessage(systemPrompt));
    }
  }
  if (prompt.examples) {
    for (const { input, output } of prompt.examples) {
      messages.push(new UserMessage(input.content));
      messages.push(new AssistantMessage(output.content));
    }
  }
  if (prompt.history) {
    for (const message of prompt.history) {
      messages.push(new OpenAIMessageImpl(message));
    }
  }
  for (const message of prompt.messages) {
    messages.push(new OpenAIMessageImpl(message));
  }
  return messages;
}

function createSystemPrompt(context: ChatRequestContext, functions?: Function[]) {
  const systemPrompt: string[] = [];
  if (context.system_prompt) {
    systemPrompt.push(context.system_prompt);
  }
  if (functions) {
    systemPrompt.push('The following tools are available:');
    for (const func of functions) {
      const tooldef: string[] = [];
      tooldef.push('Name: ' + func.name);
      if (func.description) {
        tooldef.push('Description: ' + func.description);
      }
      if (func.parameters) {
        try {
          const schema = new SchemishConverter(func.parameters);
          tooldef.push('Parameters: ' + schema.convert());
        } catch (err) {
          logger.error(err, err.stack);
          // skip
        }
      }
      systemPrompt.push(tooldef.join('\n'));
    }
  }
  if (context.additional_context) {
    const { content, chunks = [] } = context.additional_context;
    if (content) {
      systemPrompt.push(content);
    }
    for (const chunk of chunks) {
      let ctx = chunk.content;
      if (chunk.citation_metadata) {
        const sources = chunk.citation_metadata.citation_sources.map(s => s.uri);
        ctx += ` Sources: [${sources.join(', ')}]`;
      }
      systemPrompt.push(ctx);
    }
  }
  if (systemPrompt.length) {
    return systemPrompt.join(PARA_DELIM);
  }
  return null;
}

/*** ************/
