import uuid from 'uuid';

import logger from '../../logger';
import { getMimetype } from '../../utils';

import { SchemishConverter } from './Schemish';
import {
  AnthropicChatCompletionResponse
} from '../models/anthropic_types';
import {
  CohereChatCompletionResponse,
} from '../models/cohere_types';
import {
  GeminiChatResponse,
  GeminiContent,
  GeminiTools,
} from '../models/gemini_types';
import {
  MistralEmbeddingResponse,
} from '../models/mistral_types';
import {
  OpenAIMessageImpl,
  UserMessage,
  AssistantMessage,
  SystemMessage,
  FunctionMessage,
} from '../models/openai';
import {
  OpenAIMessage,
  OpenAICompletionResponse,
  OpenAIChatCompletionResponse,
} from '../models/openai_types';
import {
  PaLMChatResponse,
  PaLMCompletionResponse,
  PaLMEmbeddingResponse,
  PaLMExample,
  PaLMMessage,
} from '../models/vertexai_types';

// TODO get prompts from store
import { getPromptTemplate } from './prompt_template_variation_1';
import { getPromptTemplate as buildToolsPrompt } from './prompt_template_variation_3';

export const PARA_DELIM = '\n\n';

const OPENAI_MODELS_SUPPORTING_FUNCTIONS = [
  'gpt-4',
  'gpt-4-1106-preview',
  'gpt-4-0613',
  'gpt-3.5-turbo',
  'gpt-3.5-turbo-1106',
  'gpt-3.5-turbo-0613',
];

const OPENAI_MODELS_SUPPORTING_PARALLEL_FUNCTION_CALLING = [
  'gpt-4-1106-preview',
  'gpt-3.5-turbo-1106',
];

/*** universal superset ************/

interface CitationSource {
  start_index?: number;  // Start of segment of the response that is attributed to this source. Index indicates the start of the segment, measured in bytes.
  end_index?: number;  // End of the attributed segment, exclusive.
  uri?: string;  // URI that is attributed as a source for a portion of the text.
  license_?: string;  // License for the GitHub project that is attributed as a source for segment.  License info is required for code citations.
}

export interface CitationMetadata {
  citation_sources: CitationSource[];
}

interface ContextChunk {
  content: string;
  author?: string;
  citation_metadata?: CitationMetadata;
}

interface AdditionalContext {
  content?: string;
  chunks?: ContextChunk[];
}

export interface ChatRequestContext {
  system_prompt?: string;
  additional_context?: AdditionalContext;
}

export enum MessageRole {
  system = 'system',
  user = 'user',
  assistant = 'assistant',
  function = 'function',
  tool = 'tool',
}

export interface FunctionCall {
  name: string;  // The name of the function to call.
  arguments: any;  // The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema.
}

export interface TextContent {
  type: string;
  text: string;
}

interface ImageURL {
  url: string;  // Either a URL of the image or the base64 encoded image data.
  detail?: string;  // Specifies the detail level of the image. Defaults to auto
}

export interface ImageContent {
  type: string;
  image_url: ImageURL;
  objectName?: string;
}

export type ContentObject = TextContent | ImageContent;

export type ContentType = string | string[] | ContentObject[];

export interface Message<T = ContentType> {
  role: MessageRole;
  content: T;
  name?: string;
  function_call?: FunctionCall;
  citation_metadata?: CitationMetadata;
  final?: boolean;
}

interface FewShotLearningExample {
  input: Message;
  output: Message;
}

interface ChatPrompt {
  context?: ChatRequestContext;
  examples?: FewShotLearningExample[];
  history?: Message[];
  messages: Message[];
}

export interface ModelParams {
  max_tokens?: number;  // The maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length. Defaults to inf.
  n?: number;  // How many chat completion choices to generate for each input message. Defaults to 1.
  temperature?: number;  // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Defaults to 1. Alter this or top_p but not both.
  top_p?: number;  // An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Defaults to 1. Alter this or temperature but not both.
  stop?: string | string[];  // Up to 4 sequences where the API will stop generating further tokens. Defaults to null.
  presence_penalty?: number;  // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to 0.
  frequency_penalty?: number;  // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Defaults to 0.
  logit_bias?: object;  // Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. Defaults to null.
}

interface ChatModelParams extends ModelParams {
  top_k?: number;
}

export interface Function {
  name: string;  // The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
  description?: string;  // A description of what the function does, used by the model to choose when and how to call the function.
  parameters: any;  // JSONSchema document that describes the function and arguments.
}

export enum FunctionCallType {
  none = 'none',  // "none" means the model does not call a function, and responds to the end-user.
  auto = 'auto',  // "auto" means the model can pick between an end-user or calling a function.
}

enum HarmBlockThreshold {
  HARM_BLOCK_THRESHOLD_UNSPECIFIED = 0,
  BLOCK_LOW_AND_ABOVE = 1,
  BLOCK_MEDIUM_AND_ABOVE = 2,
  BLOCK_ONLY_HIGH = 3,
  BLOCK_NONE = 4,
}

export interface SafetySetting {
  category: string;  // The category for this setting.
  threshold: HarmBlockThreshold;  // Controls the probability threshold at which harm is blocked.
}

export interface ChatRequest {
  model: string;
  prompt: ChatPrompt;
  model_params?: ChatModelParams;
  functions?: Function[];  // A list of functions the model may generate JSON inputs for.
  function_call?: FunctionCallType | object;
  best_of?: number;
  stream?: boolean;
  user?: string;
  safety_settings?: SafetySetting[];
  safe_mode?: boolean;
  random_seed?: number;
}

export interface SafetyRating {
  category: string;
  probability: string;
}

export interface LogProbs {
  text_offset?: number[];
  token_logprobs: number[];
  tokens: string[];
  top_logprobs?: Record<string, number>[];
}

interface ChatCompletionChoice {
  finish_reason?: string;
  index: number;
  message: Message;
  safety_ratings?: SafetyRating[];  // Ratings for the safety of a response. There is at most one rating per category.
  logprobs?: LogProbs;
}

export interface ChatCompletionUsage {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
}

enum BlockedReason {
  BLOCKED_REASON_UNSPECIFIED = 0,
  SAFETY = 1,
  OTHER = 2,
}

export interface ContentFilter {
  reason: BlockedReason;
  message: string;
}

export interface SafetyFeedback {
  rating: SafetyRating;  // Safety rating evaluated from content.
  setting: SafetySetting;  // Safety setting applied to the request.
}

export interface ChatResponse {
  id: string;
  created: Date;
  model?: string;
  choices: ChatCompletionChoice[];
  usage?: ChatCompletionUsage;
  n: number;
  filters?: ContentFilter[];
  safetyFeedback?: SafetyFeedback[];
}

export interface ImageMetadata {
  quality: string;
  width: number;
  height: number;
}

interface SystemInput {
  args?: any;
  messages?: Message[];
  history?: Message[];
  extraSystemPrompt?: string;
}

export interface ResponseMetadata {
  provider: string;
  functionId: number;
  functionName: string;
  prompts: Message[];
  images: ImageMetadata[];
  costComponents: Array<any>;
  totalCost: number;
  creditBalance: number;
  modelInput: any;
  modelUserInputText: string;
  systemInput: SystemInput;
  outputType: string;
  systemOutput: Message;
  systemOutputText: string;
  modelOutput: Message;
  modelOutputText: string;
  implementation: string;
  promptTokens: number;
  completionTokens: number;
  totalTokens: number;
}

export interface EmbeddingRequest {
  model?: string;  // ID of the model to use. 
  input: string | string[];  // Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. Each input must not exceed the max input tokens for the model (8191 tokens for text-embedding-ada-002).
  user?: string;  // A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
}

interface EmbeddingObject {
  index: number;  // The index of the embedding in the list of embeddings.
  object: string;  // The object type, which is always "embedding".
  embedding: number[];  // The embedding vector, which is a list of floats. The length of vector depends on the model.
}

export interface EmbeddingUsage {
  prompt_tokens: number;
  total_tokens: number;
}

export interface EmbeddingResponse {
  object: string;  // The object type, which is always "list".
  data: EmbeddingObject[];
  model: string;
  usage: EmbeddingUsage;
}

type ToolsPromptBuilder = (toolDefinitions: string, toolKeys: string) => string[];

/*** ************/

/*** translate to anthropic ************/

export function toAnthropicChatRequest(request: ChatRequest) {
  const {
    model,
    model_params,
    stream,
  } = request;
  const {
    temperature,
    top_k,
    top_p,
    stop = [],
    max_tokens,
  } = model_params;
  const messages = createOpenAIMessages(request.prompt);
  const prompt =
    PARA_DELIM + 'Human: ' +
    messages.map(m => m.content).join(PARA_DELIM) + PARA_DELIM +
    'Assistant:'
    ;
  const stop_sequences = ['\\n\\nHuman:', ...stop];
  return {
    modelId: model,
    body: {
      prompt,
      max_tokens_to_sample: max_tokens,
      temperature,
      top_k,
      top_p,
      stop_sequences,
      stream,
    }
  };
}

export async function fromAnthropicChatResponse(response: AnthropicChatCompletionResponse, parserService) {
  const {
    completion,
    stop_reason,
    model,
  } = response;
  let choices: ChatCompletionChoice[];

  const { json } = await parserService.parse('json', completion);
  const { action, action_input } = json;
  if (action) {
    if (action === 'Final Answer') {
      choices = [
        {
          finish_reason: stop_reason,
          index: 0,
          message: {
            role: MessageRole.assistant,
            content: action_input,
            final: true,
          },
        }
      ];
    } else {
      const args = { input: action_input };
      choices = [
        {
          finish_reason: stop_reason,
          index: 0,
          message: {
            role: MessageRole.function,
            content: null,
            function_call: {
              name: action,
              arguments: JSON.stringify(args),
            },
          },
        }
      ];
    }
  } else {
    choices = [
      {
        finish_reason: stop_reason,
        index: 0,
        message: {
          role: MessageRole.assistant,
          content: completion,
        },
      }
    ];
  }
  return {
    id: uuid.v4(),
    created: new Date(),
    model,
    n: choices.length,
    choices,
  };
}

/*** ************/

/*** translate to cohere ************/

export function toCohereChatRequest(request: ChatRequest) {
  const {
    model,
    model_params,
    stream,
  } = request;
  const {
    n,
    temperature,
    top_k,
    top_p = 0.75,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  } = model_params;
  const messages = createOpenAIMessages(request.prompt);
  const prompt = messages.map(m => m.content).join(PARA_DELIM);
  return {
    modelId: model,
    body: {
      prompt,
      // model,
      num_generations: n,
      stream,
      max_tokens,
      temperature,
      stop_sequences: stop?.length ? stop : undefined,
      k: top_k,
      p: Math.max(top_p, 0.99),

      // doesn't appear supported in Bedrock API - "ValidationException: Malformed input request: extraneous key [presence_penalty]"
      // frequency_penalty,
      // presence_penalty,

      logit_bias,
    }
  };
}

export async function fromCohereChatResponse(response: CohereChatCompletionResponse, parserService) {
  const {
    id,
    prompt,
    generations,
    meta,
  } = response;
  const content = generations[0].text;
  let choices: ChatCompletionChoice[];
  const { json } = await parserService.parse('json', content);
  const { action, action_input } = json;
  if (action) {
    if (action === 'Final Answer') {
      choices = [
        {
          index: 0,
          message: {
            role: MessageRole.assistant,
            content: action_input,
            final: true,
          },
        }
      ];
    } else {
      const args = { input: action_input };
      choices = [
        {
          index: 0,
          message: {
            role: MessageRole.function,
            content: null,
            function_call: {
              name: action,
              arguments: JSON.stringify(args),
            },
          },
        }
      ];
    }
  } else {
    choices = generations.map(g => ({
      index: g.index,
      message: {
        role: MessageRole.assistant,
        content: g.text,
        finish_reason: g.finish_reason,
      },
      logprobs: {
        tokens: g.token_likelihoods?.map(t => t.token),
        token_logprobs: g.token_likelihoods?.map(t => t.likelihood),
      },
    }));
  }
  return {
    id,
    created: new Date(),
    model: meta?.api_version?.version,
    n: generations.length,
    choices,
  };
}

/*** ************/

/*** translate to gemini ************/

function getGeminiRole(role: MessageRole) {
  switch (role) {
    case MessageRole.system:
    case MessageRole.user:
      return 'user';

    case MessageRole.assistant:
      return 'model';

    default:
  }
}

function getGeminiContentParts(content: ContentType) {
  if (typeof content === 'string') {
    return [{ text: content }];
  }
  if (Array.isArray(content) && content.length) {
    if (typeof content[0] === 'string') {
      return (content as string[]).map(text => ({ text }));
    }
    return (content as ContentObject[]).map(c => {
      if (c.type === 'text') {
        return { text: (c as TextContent).text };
      } else {
        const imageContent = c as ImageContent;
        const file_uri = imageContent.image_url.url;
        const mime_type = getMimetype(imageContent.objectName);
        return { file_data: { mime_type, file_uri } };
      }
    });
  }
  return [];
}

function createGeminiContents(prompt: ChatPrompt) {
  const contents: GeminiContent[] = [];
  if (prompt.history) {
    for (const message of prompt.history) {
      if (message.role !== 'function') {
        contents.push({
          role: getGeminiRole(message.role),
          parts: getGeminiContentParts(message.content),
        });
      }
    }
  }
  if (prompt.examples) {
    for (const { input, output } of prompt.examples) {
      contents.push({
        role: 'user',
        parts: getGeminiContentParts(input.content),
      });
      contents.push({
        role: 'model',
        parts: getGeminiContentParts(output.content),
      });
    }
  }
  for (const message of prompt.messages) {
    if (message.role !== 'function') {
      contents.push({
        role: getGeminiRole(message.role),
        parts: getGeminiContentParts(message.content),
      });
    }
  }
  return contents;
}

function createGeminiVisionContents(prompt: ChatPrompt) {
  const contents: GeminiContent[] = [];
  for (const message of prompt.messages) {
    if (message.role !== 'function') {
      contents.push({
        role: getGeminiRole(message.role),
        parts: getGeminiContentParts(message.content),
      });
    }
  }
  return contents;
}

export function toGeminiChatRequest(request: ChatRequest) {
  const {
    model,
    model_params,
    safe_mode,
    prompt,
    functions,
  } = request;
  const {
    temperature,
    top_p,
    top_k,
    max_tokens,
    stop,
  } = model_params;
  let contents: GeminiContent[];
  if (model === 'gemini-pro-vision') {
    contents = createGeminiVisionContents(prompt);
  } else {
    contents = createGeminiContents(prompt);
  }
  let tools: GeminiTools;
  if (functions) {
    tools = {
      function_declarations: functions,
    };
  }
  let safety_settings: SafetySetting[];
  if (safe_mode) {
    safety_settings = [
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 2,  // medium probability and above
      },
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 2,
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 2,
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 2,
      },
    ];
  }
  return {
    model,
    contents,
    generation_config: {
      temperature,
      top_p,
      top_k,
      candidate_count: 1,  // must be 1
      max_output_tokens: max_tokens,
      stop_sequences: stop,
    },
    tools,
    safety_settings,
  };
}

export function fromGeminiChatResponse(response: GeminiChatResponse) {
  try {
    const choices = response.candidates.map((c, i) => {
      let citation_metadata: CitationMetadata;
      if (c.citationMetadata) {
        const citation_sources = c.citationMetadata.citations.map(c => ({
          start_index: c.startIndex,
          end_index: c.endIndex,
          uri: c.uri,
          license_: c.license,
        }));
        citation_metadata = { citation_sources };
      }
      return {
        index: i,
        finish_reason: c.finishReason.toString(),
        message: {
          role: MessageRole.assistant,
          content: c.content.parts[0].text,
          citation_metadata,
        },
        safety_ratings: c.safetyRatings?.map(r => ({
          category: r.category,
          probability: r.probability,
        })),
      };
    });
    let usage;
    if (response.usageMetadata) {
      const { promptTokenCount, candidatesTokenCount, totalTokenCount } = response.usageMetadata;
      usage = {
        completion_tokens: candidatesTokenCount,
        prompt_tokens: promptTokenCount,
        total_tokens: totalTokenCount,
      };
    }
    return {
      id: uuid.v4(),
      created: new Date(),
      choices,
      usage,
      n: choices.length,
    }
  } catch (err) {
    logger.error(err, err.stack);
    throw err;
  }
}

/*** ************/

/*** translate to llamaapi ************/

export function toLlamaApiChatRequest(request: ChatRequest) {
  const {
    functions,
    function_call,
    stream,
  } = request;
  const messages = createOpenAIMessages(request.prompt);
  return {
    messages,
    functions,
    function_call,
    stream,
  };
}

export function fromLlamaApiChatResponse(response: OpenAIChatCompletionResponse) {
  const {
    choices,
  } = response;
  return {
    id: uuid.v4(),
    created: new Date(),
    n: choices.length,
    choices: choices.map(c => ({
      finish_reason: c.finish_reason,
      index: c.index,
      message: {
        role: c.message.role,
        content: c.message.content,
        function_call: c.message.function_call,
      }
    })),
  };
}

/*** ************/

/*** translate to mistral ************/

export function toMistralChatRequest(request: ChatRequest) {
  const {
    model,
    model_params,
    stream,
    safe_mode,
    random_seed,
  } = request;
  const {
    temperature,
    top_p,
    max_tokens,
  } = model_params;
  const messages = createOpenAIMessages(request.prompt);
  return {
    model,
    messages,
    temperature,
    top_p,
    max_tokens,
    stream,
    safe_mode,
    random_seed,
  };
}

export function toMistralEmbeddingRequest(request: EmbeddingRequest) {
  const { model, input } = request;
  if (typeof input === 'string') {
    return {
      model: model || 'mistral-embed',
      input: [input],
      encoding_format: 'float',
    };
  }
  return {
    model: model || 'mistral-embed',
    input,
    encoding_format: 'float',
  };
}

export function fromMistralEmbeddingResponse(response: MistralEmbeddingResponse) {
  return response.data[0];
}

/*** ************/

/*** translate to openai ************/

export function toOpenAIChatRequest(request: ChatRequest) {
  const {
    model,
    model_params,
    stream,
    user,
  } = request;
  const {
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  } = model_params;
  let functions: Function[];
  let function_call: FunctionCallType | object;
  let messages: Message[];
  if (OPENAI_MODELS_SUPPORTING_FUNCTIONS.includes(model)) {
    functions = request.functions;
    function_call = request.function_call;
    messages = createOpenAIMessages(request.prompt);
  } else {
    logger.debug('model "%s" doesn\'t support function calling', model);
    logger.debug('functions:', request.functions);
    messages = createOpenAIMessages(request.prompt, request.functions, buildToolsPrompt);
  }
  return {
    model,
    messages,
    functions,
    function_call,
    stream,
    user,
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  };
}

export async function fromOpenAIChatResponse(response: OpenAIChatCompletionResponse, parserService) {
  const {
    id,
    created,
    model,
    usage,
  } = response;
  let choices: ChatCompletionChoice[];
  if (OPENAI_MODELS_SUPPORTING_FUNCTIONS.includes(model)) {
    choices = response.choices.map(c => ({
      finish_reason: c.finish_reason,
      index: c.index,
      message: {
        role: c.message.role,
        content: c.message.content,
        name: c.message.name,
        function_call: c.message.function_call,
      }
    }))
  } else {
    const candidates = response.choices;
    if (candidates.length) {
      const candidate = candidates[0];
      const message = candidate.message;
      const { json } = await parserService.parse('json', message.content);
      const { action, action_input } = json;
      if (action) {
        if (action === 'Final Answer') {
          choices = [
            {
              finish_reason: candidate.finish_reason,
              index: candidate.index,
              message: {
                role: MessageRole.assistant,
                content: action_input,
                name: message.name,
                citation_metadata: message.citation_metadata,
                final: true,
              },
            }
          ];
        } else {
          choices = [
            {
              finish_reason: candidate.finish_reason,
              index: candidate.index,
              message: {
                role: MessageRole.function,
                content: null,
                name: message.name,
                function_call: {
                  name: action,
                  arguments: JSON.stringify(action_input),
                },
                citation_metadata: message.citation_metadata,
              }
            }
          ];
        }
      } else {
        choices = response.choices.map(c => ({
          finish_reason: c.finish_reason,
          index: c.index,
          message: {
            role: c.message.role,
            content: c.message.content,
            name: c.message.name,
            function_call: c.message.function_call,
          }
        }))
      }
    } else {
      choices = [
        {
          index: 0,
          message: {
            role: MessageRole.assistant,
            content: 'No response from model',
            final: true,
          },
        }
      ];
    }
  }
  return {
    id,
    created,
    model,
    n: choices.length,
    choices,
    usage,
  };
}

export function toOpenAICompletionRequest(request: ChatRequest) {
  const {
    functions,
    model,
    model_params,
    best_of,
    stream,
    user,
  } = request;
  const messages = createOpenAIMessages(request.prompt, functions);
  const prompt = messages.map(m => m.content).join(PARA_DELIM);
  const {
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
  } = model_params;
  return {
    model,
    prompt,
    stream,
    user,
    temperature,
    top_p,
    n,
    stop,
    max_tokens,
    presence_penalty,
    frequency_penalty,
    logit_bias,
    best_of,
  };
}

export async function fromOpenAICompletionResponse(response: OpenAICompletionResponse, parserService: any) {
  const {
    id,
    created,
    model,
    usage,
  } = response;
  let choices: ChatCompletionChoice[];
  if (response.choices.length) {
    const { json } = await parserService.parse('json', response.choices[0].text);
    const { action, action_input } = json;
    if (action) {
      if (action === 'Final Answer') {
        choices = [
          {
            finish_reason: response.choices[0].finish_reason,
            index: 0,
            message: {
              role: MessageRole.assistant,
              content: action_input,
              final: true,
            },
            logprobs: response.choices[0].logprobs,
          }
        ];
      } else {
        choices = [
          {
            finish_reason: response.choices[0].finish_reason,
            index: 0,
            message: {
              role: MessageRole.function,
              content: null,
              function_call: {
                name: action,
                arguments: JSON.stringify(action_input),
              },
            },
            logprobs: response.choices[0].logprobs,
          }
        ];
      }
    } else {
      choices = response.choices.map(c => ({
        finish_reason: c.finish_reason,
        index: c.index,
        message: {
          role: MessageRole.assistant,
          content: c.text,
        },
        logprobs: c.logprobs,
      }));
    }
  } else {
    choices = [
      {
        index: 0,
        message: {
          role: MessageRole.assistant,
          content: 'No response from model',
          final: true,
        },
      }
    ];
  }
  return {
    id,
    created,
    model,
    choices,
    n: choices.length,
    usage,
  };
}

/*** ************/

/*** translate to vertexai ************/

function mapPaLMMessage(message: Message) {
  return {
    author: message.name,
    content: message.content as string,
    citation_metadata: message.citation_metadata,
  };
}

function getPaLMMessages(messages: Message[], filterFunction: any) {
  return messages.filter(filterFunction).map(mapPaLMMessage);
}

function splitSystemMessages(messages: Message[]) {
  return [
    getPaLMMessages(messages, (m: Message) => m.role === 'system'),
    getPaLMMessages(messages, (m: Message) => m.role !== 'system')
  ];
}

export function toVertexAIChatRequest(request: ChatRequest) {
  const {
    prompt,
    model,
    model_params,
    functions,
  } = request;
  const {
    temperature,
    top_k,
    top_p,
    n,
  } = model_params;
  const systemMessages: PaLMMessage[] = [];
  const messages: PaLMMessage[] = [];
  if (prompt.history) {
    let [sysmsgs, msgs] = splitSystemMessages(prompt.history);
    systemMessages.push(...sysmsgs);
    messages.push(...msgs);
  }
  if (prompt.messages) {
    let [sysmsgs, msgs] = splitSystemMessages(prompt.messages);
    systemMessages.push(...sysmsgs);
    messages.push(...msgs);
  }
  let examples: PaLMExample[];
  if (prompt.examples) {
    examples = prompt.examples.map(({ input, output }) => ({
      input: toPaLMMessage(input),
      output: toPaLMMessage(output),
    }));
  }
  const context = createSystemPrompt(prompt.context, systemMessages as SystemMessage[], functions);
  return {
    model,
    prompt: {
      context,
      examples,
      messages,
    },
    temperature,
    candidate_count: n,
    top_p,
    top_k,
  };
}

export async function fromVertexAIChatResponse(response: PaLMChatResponse, parserService: any) {
  const {
    candidates,
    filters,
  } = response;
  let choices: ChatCompletionChoice[];
  if (candidates.length) {
    const { json } = await parserService.parse('json', candidates[0].content);
    const { action, action_input } = json;
    if (action) {
      if (action === 'Final Answer') {
        choices = [
          {
            index: 0,
            message: {
              role: MessageRole.assistant,
              content: action_input,
              name: candidates[0].author,
              citation_metadata: candidates[0].citation_metadata,
              final: true,
            },
          }
        ];
      } else {
        const args = { input: action_input };
        choices = [
          {
            index: 0,
            message: {
              role: MessageRole.function,
              content: null,
              name: candidates[0].author,
              function_call: {
                name: action,
                arguments: JSON.stringify(args),
              },
              citation_metadata: candidates[0].citation_metadata,
            }
          }
        ];
      }
    } else {
      choices = candidates.map((c, i) => ({
        index: i,
        message: {
          role: MessageRole.assistant,
          content: c.content,
          name: c.author,
          citation_metadata: c.citation_metadata,
        }
      }));
    }
  } else {
    choices = [
      {
        index: 0,
        message: {
          role: MessageRole.assistant,
          content: 'No response from model',
          final: true,
        },
      }
    ];
  }
  return {
    id: uuid.v4(),
    created: new Date(),
    choices,
    n: candidates.length,
    filters,
  };
}

export function toVertexAICompletionRequest(request: ChatRequest) {
  const {
    functions,
    model,
    model_params,
    safety_settings,
  } = request;
  const messages = createOpenAIMessages(request.prompt, functions);
  const text = messages.map(m => m.content).join(PARA_DELIM);
  const {
    temperature,
    top_p,
    top_k,
    n,
    stop,
    max_tokens,
  } = model_params;
  return {
    model,
    prompt: { text },
    temperature,
    candidate_count: n,
    max_output_tokens: max_tokens,
    top_p,
    top_k,
    safety_settings,
    stop_sequences: stop,
  };
}

export async function fromVertexAICompletionResponse(response: PaLMCompletionResponse, parserService: any) {
  const {
    candidates,
    filters,
    safety_feedback,
  } = response;
  let choices: ChatCompletionChoice[];
  if (candidates.length) {
    const { json } = await parserService.parse('json', candidates[0].output);
    const { action, action_input } = json;
    if (action) {
      if (action === 'Final Answer') {
        choices = [
          {
            index: 0,
            message: {
              role: MessageRole.assistant,
              content: action_input,
              citation_metadata: candidates[0].citation_metadata,
              final: true,
            },
            safety_ratings: candidates[0].safety_ratings,
          }
        ];
      } else {
        const args = { input: action_input };
        choices = [
          {
            index: 0,
            message: {
              role: MessageRole.function,
              content: null,
              function_call: {
                name: action,
                arguments: JSON.stringify(args),
              },
              citation_metadata: candidates[0].citation_metadata,
            },
            safety_ratings: candidates[0].safety_ratings,
          }
        ];
      }
    } else {
      choices = candidates.map((c, i) => ({
        index: i,
        message: {
          role: MessageRole.assistant,
          content: c.output,
          citation_metadata: c.citation_metadata,
        },
        safety_ratings: c.safety_ratings,
      }));
    }
  } else {
    choices = [
      {
        index: 0,
        message: {
          role: MessageRole.assistant,
          content: 'No response from model',
          final: true,
        },
      }
    ];
  }
  return {
    id: uuid.v4(),
    created: new Date(),
    choices,
    n: candidates.length,
    safetyFeedback: safety_feedback,
    filters,
  };
}

export function toVertexAIEmbeddingRequest(request: EmbeddingRequest) {
  const { model, input } = request;
  let text: string;
  if (Array.isArray(input)) {
    text = input.join(PARA_DELIM);
  } else {
    text = input;
  }
  return { model, text };
}

export function fromVertexAIEmbeddingResponse(response: PaLMEmbeddingResponse) {
  return {
    index: 0,
    object: 'embedding',
    embedding: response.embedding.value,
  };
}

function toPaLMMessage(message: Message) {
  return {
    author: message.name,
    content: message.content as string,
    citation_metadata: message.citation_metadata,
  };
}

/*** ************/

/*** utility ************/

function createOpenAIMessages(
  prompt: ChatPrompt,
  functions?: Function[],
  toolsPromptBuilder?: ToolsPromptBuilder
) {
  const systemMessages: OpenAIMessage[] = [];
  const messages: OpenAIMessage[] = [];
  if (prompt.history) {
    for (const message of prompt.history) {
      if (message.role === 'system') {
        systemMessages.push(new OpenAIMessageImpl<string>(message as SystemMessage));
      } else if (message.role === 'function') {
        messages.push(new FunctionMessage(makeObservation(message.content), message.name));
      } else {
        messages.push(new OpenAIMessageImpl(message));
      }
    }
  }
  if (prompt.examples) {
    for (const { input, output } of prompt.examples) {
      messages.push(new UserMessage(input.content));
      messages.push(new AssistantMessage(output.content as string));
    }
  }
  for (const message of prompt.messages) {
    if (message.role === 'system') {
      systemMessages.push(new OpenAIMessageImpl<string>(message as SystemMessage));
    } else if (message.role === 'function') {
      messages.push(new FunctionMessage(makeObservation(message.content), message.name));
    } else {
      messages.push(new OpenAIMessageImpl(message));
    }
  }
  const systemPrompt = createSystemPrompt(
    prompt.context,
    systemMessages as SystemMessage[],
    functions,
    toolsPromptBuilder
  );
  if (systemPrompt) {
    return [new SystemMessage(systemPrompt), ...messages];
  }
  return messages;
}

function getContextPrompt(context: ChatRequestContext) {
  const systemPrompt: string[] = [];
  if (context.system_prompt) {
    systemPrompt.push(context.system_prompt);
  }
  if (context.additional_context) {
    const { content, chunks = [] } = context.additional_context;
    if (content) {
      systemPrompt.push(content);
    }
    for (const chunk of chunks) {
      let ctx = chunk.content;
      if (chunk.citation_metadata) {
        const sources = chunk.citation_metadata.citation_sources.map(s => s.uri);
        ctx += ` Sources: [${sources.join(', ')}]`;
      }
      systemPrompt.push(ctx);
    }
  }
  return systemPrompt;
}

export function getToolDefinitions(functions: Function[]) {
  const d: string[] = [];
  for (const f of functions) {
    try {
      const schema = new SchemishConverter(f.parameters);
      const args = schema.convert();
      d.push(`${f.name}: ${f.description}, args: ${args}`);
    } catch (err) {
      logger.error(err, err.stack);
      // skip tool
    }
  }
  return d.join('\n');
}

function getFunctionPrompts(functions: Function[], toolsPromptBuilder?: ToolsPromptBuilder) {
  if (!toolsPromptBuilder) {
    // get default
    toolsPromptBuilder = getPromptTemplate;
  }
  const toolDefinitions = getToolDefinitions(functions);
  const toolKeys = functions.map(f => f.name).join(', ');
  return toolsPromptBuilder(toolDefinitions, toolKeys);
}

function createSystemPrompt(
  context: ChatRequestContext,
  systemMessages: SystemMessage[],
  functions?: Function[],
  toolsPromptBuilder?: ToolsPromptBuilder
) {
  const systemPrompt: string[] = [];
  if (context) {
    systemPrompt.push(...getContextPrompt(context));
  }
  if (systemMessages?.length) {
    systemPrompt.push(...systemMessages.map(m => m.content));
  }
  if (functions?.length) {
    systemPrompt.push(...getFunctionPrompts(functions, toolsPromptBuilder));
  }
  if (systemPrompt.length) {
    return systemPrompt.join(PARA_DELIM);
  }
  return null;
}

export function fillContent(templateFiller: any, args: any, content: ContentType) {
  if (typeof content === 'string') {
    return templateFiller(content, args);
  }
  if (Array.isArray(content) && content.length) {
    if (typeof content[0] === 'string') {
      return (content as string[]).map(c => templateFiller(c, args));
    }
    return (content as ContentObject[]).map(c => {
      if (c.type === 'text') {
        return { ...c, text: templateFiller((c as TextContent).text, args) };
      }
      return c;
    });
  }
  return content;
}

/**
 * @param content 
 * @returns 
 */
export function convertContentTypeToString(content: ContentType) {
  if (typeof content === 'string') {
    return content;
  }
  if (Array.isArray(content) && content.length) {
    if (typeof content[0] === 'string') {
      return (content as string[]).join('\n\n');
    }
    return (content as ContentObject[])
      .filter(c => c.type === 'text')
      .map((c: TextContent) => c.text)
      .join('\n\n');
  }
  return '';
}

function makeObservation(observation: ContentType) {
  return convertContentTypeToString(observation);
  // return 'Observation: ' + observation + '\nThought: ';
}

export function getText(messages: Message[]) {
  return messages
    .map(m => convertContentTypeToString(m.content))
    .join('\n\n');
}

/*** ************/
